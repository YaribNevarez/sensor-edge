\title {CNN Sensor Analytics with Hybrid-Float6 on Low-Power Resource-Constrained Embedded FPGAs.}


\tfootnote{This work is funded by the Consejo Nacional de Ciencia y Tecnologia - CONACYT}

\markboth
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}
{Author \headeretal: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS}

\corresp{Corresponding author: Yarib Nevarez (e-mail: nevarez@item.uni-bremen.de).}

\begin{abstract}
The use of artificial intelligence (AI) in sensor analytics is entering a new era based on the use of ubiquitous embedded connected devices. This transformation requires the adoption of design techniques that reconcile accurate results with sustainable system architectures. As such, improving the efficiency of AI hardware engines as well as machine learning (ML) format compatibility must be considered. In this paper, we present the Hybrid-Float6 (HF6) quantization, where feature maps and weights are represented by 32-bit and 6-bit floating-point (FP), respectively. The 6-bit FP representation uses 4-bit exponent and 1-bit mantissa. This approach enables low-power multiply-accumulate (MAC) implementations by reducing the FP mantissa multiplication to a flag operation. We present a quantization-aware training (QAT) method that preserves ML accuracy and, in some cases, improves it. We demonstrate this concept in 2D convolution layers as compute-bound operations. We present a low-power tensor processor (TP) implementing a pipelined vector dot-product. The HF6 quantization is ML backward compatible. The proposed hardware/software architecture is compliant with TensorFlow (TF) Lite. We evaluate the applicability of our approach with a structural health monitoring (SHM) data analytics application for anomaly localization. The embedded hardware/software framework is demonstrated on XC7Z007S. The proposed TP achieves a peak power efficiency and acceleration of $5.7$ GFLOPS/s/W and $48.3\times$, respectively.
\end{abstract}

\begin{keywords}
Convolutional neural networks, structural health monitoring, hardware accelerator, TensorFlow Lite, embedded systems, FPGA, custom floating-point
\end{keywords}

\titlepgskip=-15pt

\maketitle