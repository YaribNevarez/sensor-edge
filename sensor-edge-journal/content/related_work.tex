\section{Related work}
\label{sec:related_work}
In the literature we find plenty of hardware architectures
dedicated to CNN accelerators implemented in FPGA and ASIC. Most of the research implements fixed-point quantization, and very limited research focuses on floating-point. Moreover, to the best of our knowledge, there is no research work focusing on floating-point inference for low-power embedded applications.


\subsection{Hybrid Custom Floating-Point}
In \cite{lai2017deep}, Liangzhen Lai et al. proposed a mixed data representation with floating-point for weights and fixed-point for activations. In \cite{settle2018quantizing}, Sean O. Settle et al. presented an 8-bit floating-point quantization scheme, which needs an extra inference batch to compensate for quantization error. However, \cite{lai2017deep} and \cite{settle2018quantizing} did not present a hardware architecture for their approaches. In \cite{lian2019high}, Xiaocong Lian et al. proposed an accelerator with optimized block floating-point (BFP), in this design the activations and weights are represented by 16-bit and 8-bit floating-point formats, respectively. This design is demonstrated on Xilinx VC709 PCIe evaluation board. This implementation achieves throughput and power efficiency of 760.83 GOP/s and 82.88 GOP/s/W, respectively.

\subsection{Low-Precision Floating-Point}
in \cite{mei2017200mhz}, Chunsheng Mei et al. presented a hardware accelerator for VGG16 model with 16-bit floating-point. This design is demonstrated on Xilinx Virtex-7 XC7VX690T with PCIe interface. This implementation achieves throughput and power efficiency of 202.8 GFLOP/s and 18.72 GFLOP/s/W, respectively. In \cite{wu2021low}, Chen Wu et al. proposed a low-precision (8-bit) floating-point (LPFP) quantization method for FPGA-based acceleration. This design is demonstrated on Xilinx Kintex 7 and Ultrascale/Ultrascale+. This implementation achieves throughput and power efficiency of 1086.8 GOP/s and 115.4 GOP/s/W, respectively.

\subsection{Low-Power}
Two research papers have been reported hardware accelerators targeting XC7Z007S. This is the smallest device from Zynq-7000 SoC family. In \cite{meloni2019cnn}, Paolo Meloni et al. presented a CNN inference accelerator for compact and cost-optimized devices. This implementation uses fixed-point for processing light-weight CNN architectures with a power efficiency between 2.49 to 2.98 GOPS/s/W. In \cite{gao2020edgedrnn}, Chang Gao et al. presented EdgeDRNN, a recurrent neural network (RNN) accelerator for edge inference. This implementation adopts the spiking neural network (SNN) inspired delta network algorithm to exploit temporal sparsity in RNNs. However, this hardware architecture is not designed for general purpose CNN inference.